# Foggy-generation-dissertation-KCI-
í•œêµ­êµí†µëŒ€í•™êµ EVLAB KCI ë…¼ë¬¸ Foggy generation dissertation
-ì‹¬ì‚¬ì¤‘-

Depth-Aware 3D Fog Synthesis with Temporal Consistency

Official implementation of the paper
â€œDepth-Aware 3D Fog Synthesis with Temporal Consistencyâ€

ğŸ“„ Paper

Depth-Aware 3D Fog Synthesis with Temporal Consistency
(ê¹Šì´ ê¸°ë°˜ 3D ì•ˆê°œ í•©ì„±ê³¼ ì‹œê°„ì  ì¼ê´€ì„±)

This repository provides an implementation of the 3D volumetric fog synthesis framework proposed in our paper.
The method generates spatially and temporally coherent non-homogeneous fog by combining:

3D volumetric noise generation

Depth-aware transmission modeling

Temporal filtering in space-time domain

ğŸ” Abstract (Short)

Conventional fog synthesis methods are mainly limited to 2D image-space processing, resulting in insufficient depth perception and severe temporal flickering in video sequences.
This work proposes a depth-aware 3D fog synthesis method that constructs a non-homogeneous volumetric fog field in space-time and integrates it with a depth-dependent transmission model.
By applying 3D FFT-based Gaussian filtering, the proposed method ensures temporal consistency while preserving realistic volumetric appearance.

ğŸ§  Method Overview

The proposed framework consists of four main stages:

1. 3D Volumetric Fog Field Generation

Generate random Gaussian noise in space-time volume

Apply 3D FFT

Suppress high-frequency components using a 3D Gaussian filter

Recover a smooth volumetric fog density field via inverse FFT

2. Contrast & Gamma Normalization

Control fog heterogeneity using contrast and gamma parameters

Simulates varying absorption coefficients of participating media

3. Depth-Aware Transmission Modeling

Depth-dependent attenuation modeled using:

Exponential function

Sigmoid function (optional)

Non-homogeneous fog density and depth attenuation are combined multiplicatively

4. Image Rendering

Final fog image synthesized using Koschmiederâ€™s atmospheric scattering model

ğŸ“ Mathematical Formulation

The final transmission map is defined as:

Tfinalâ€‹(t,y,x)=Tbaseâ€‹(t,y,x)â‹…Tdepthâ€‹(y,x)

The foggy image is rendered as:

Ihazyâ€‹=Iâ‹…Tfinalâ€‹+A(1âˆ’Tfinalâ€‹)

where ğ´
A denotes atmospheric light.

â±ï¸ Temporal Consistency

Unlike 2D noise-based methods that generate fog independently per frame,
our approach performs space-time volumetric filtering, which:

Reduces temporal flickering

Ensures smooth fog evolution across frames

Preserves volumetric coherence in videos

ğŸ“Š Evaluation

The method was evaluated against:

Homogeneous fog synthesis

2D noise-based non-homogeneous fog synthesis

Metrics

PSNR

SSIM

Lower PSNR/SSIM after dehazing indicates more realistic and challenging fog conditions,
where the proposed method consistently shows the lowest scores, confirming higher realism.

ğŸ“ Repository Structure 
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ volumetric_noise.py
â”‚   â”œâ”€â”€ fft_filtering.py
â”‚   â”œâ”€â”€ depth_transmission.py
â”‚   â”œâ”€â”€ rendering.py
â”‚   â””â”€â”€ temporal_filter.py
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ videos/
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

ğŸ§ª Usage
python synthesize_fog.py \
  --input input_video.mp4 \
  --depth depth_map.npy \
  --output output_video.mp4

ğŸ“Œ Applications

Synthetic data generation for adverse weather

Autonomous driving simulation

Computer vision robustness evaluation

Video-based environmental effects

Citation

If you use this work, please cite:

@article{depth3dfog2024,
  title={Depth-Aware 3D Fog Synthesis with Temporal Consistency},
  author={},
  journal={},
  year={2024}
}

ğŸ“œ License

This repository is released for research and non-commercial use only,
in accordance with the paperâ€™s publication license.

âœ‰ï¸ Contact

MinhoSeo (knut-Undergraduate researcher) https://itsukiseominho.github.io/Minhoseo.github.io/ , dat ngo (professiol)
